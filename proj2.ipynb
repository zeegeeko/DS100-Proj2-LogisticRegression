{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this assignment in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). Lastly, hit **Validate**.\n",
    "\n",
    "If you worked locally, and then uploaded your work to the hub, make sure to follow these steps:\n",
    "- open your uploaded notebook **on the hub**\n",
    "- hit the validate button right above this cell, from inside the notebook\n",
    "\n",
    "These  steps should solve any issue related to submitting the notebook on the hub.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Emanuel Lucban\"\n",
    "COLLABORATORS = \"Adam Osborn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9b95f0e3f952f2a69021b9e6257b0e0",
     "grade": false,
     "grade_id": "proj2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Project 2: Spam // Ham Prediction  \n",
    "\n",
    "## Due Date: 11:59pm Sunday, April 29\n",
    "\n",
    "In this project, you will use what you've learned in class to create a classifier that can distinguish spam (junk or commercial or bulk) emails from ham (non-spam) emails. In addition to providing some skeleton code to fill in, we will evaluate your work based on your model's accuracy and your written responses in this notebook.\n",
    "\n",
    "## Score breakdown\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "Question 1 | 3\n",
    "Question 2 | 2\n",
    "Question 3a | 2\n",
    "Question 3b | 2\n",
    "Question 4 | 2\n",
    "Question 5 | 2\n",
    "Question 6 | 9\n",
    "Question 7 | 6\n",
    "Question 8 | 6\n",
    "Question 9 | 3\n",
    "Question 10 | 5\n",
    "Total | 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13d4b9efb57a66ecdde4f5c2e2bcc526",
     "grade": false,
     "grade_id": "p1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Part I - Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86334642abec944a9d6c8299e6ba5896",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\", \n",
    "        color_codes = True,\n",
    "        font_scale = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d3fe1dd9f7bc02dfe273dc477925ec5",
     "grade": false,
     "grade_id": "loading",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Loading in the Data\n",
    "\n",
    "The dataset consists of email messages and their labels (0 for ham, 1 for spam). Your labelled dataset contains 8348 labelled examples, and the evaluation set contains 1000 unlabelled examples.\n",
    "\n",
    "Run the following cells to load in the data into DataFrames.\n",
    "\n",
    "The `train` DataFrame contains labelled data that you will use to train your model. It contains three columns:\n",
    "\n",
    "1. `id`: An identifier for the training example.\n",
    "1. `subject`: The subject of the email\n",
    "1. `email`: The text of the email.\n",
    "1. `spam`: 1 if the email was spam, 0 if the email was ham (not spam).\n",
    "\n",
    "The `evaluation` DataFrame contains another set of 1000 unlabelled examples. You will predict labels for these examples and submit your predictions to Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "553fddfce24f4d89fd7ad907a7b22d28",
     "grade": false,
     "grade_id": "fetch-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using version already downloaded: Wed Apr 18 18:58:38 2018\n",
      "MD5 hash of file: 0380c4cf72746622947b9ca5db9b8be8\n",
      "Using version already downloaded: Wed Apr 18 18:58:40 2018\n",
      "MD5 hash of file: a2e7abd8c7d9abf6e6fafc1d1f9ee6bf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Subject: A&amp;L Daily to be auctioned in bankrupt...</td>\n",
       "      <td>url: http://boingboing.net/#85534171\\n date: n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Subject: Wired: \"Stronger ties between ISPs an...</td>\n",
       "      <td>url: http://scriptingnews.userland.com/backiss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Subject: It's just too small                  ...</td>\n",
       "      <td>&lt;html&gt;\\n &lt;head&gt;\\n &lt;/head&gt;\\n &lt;body&gt;\\n &lt;font siz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Subject: liberal defnitions\\n</td>\n",
       "      <td>depends on how much over spending vs. how much...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Subject: RE: [ILUG] Newbie seeks advice - Suse...</td>\n",
       "      <td>hehe sorry but if you hit caps lock twice the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            subject  \\\n",
       "0   0  Subject: A&L Daily to be auctioned in bankrupt...   \n",
       "1   1  Subject: Wired: \"Stronger ties between ISPs an...   \n",
       "2   2  Subject: It's just too small                  ...   \n",
       "3   3                      Subject: liberal defnitions\\n   \n",
       "4   4  Subject: RE: [ILUG] Newbie seeks advice - Suse...   \n",
       "\n",
       "                                               email  spam  \n",
       "0  url: http://boingboing.net/#85534171\\n date: n...     0  \n",
       "1  url: http://scriptingnews.userland.com/backiss...     0  \n",
       "2  <html>\\n <head>\\n </head>\\n <body>\\n <font siz...     1  \n",
       "3  depends on how much over spending vs. how much...     0  \n",
       "4  hehe sorry but if you hit caps lock twice the ...     0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import fetch_and_cache_gdrive\n",
    "fetch_and_cache_gdrive('1SCASpLZFKCp2zek-toR3xeKX3DZnBSyp', 'train.csv')\n",
    "fetch_and_cache_gdrive('1ZDFo9OTF96B5GP2Nzn8P8-AL7CTQXmC0', 'eval.csv')\n",
    "\n",
    "original_training_data = pd.read_csv('data/train.csv')\n",
    "evaluation = pd.read_csv('data/eval.csv')\n",
    "\n",
    "# Convert the emails to lower case as a first step to processing the text\n",
    "original_training_data['email'] = original_training_data['email'].str.lower()\n",
    "evaluation['email'] = evaluation['email'].str.lower()\n",
    "\n",
    "original_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd636166056f9007afa15fb14e1add52",
     "grade": false,
     "grade_id": "train-test",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Train-Test Split\n",
    "\n",
    "The training data we downloaded is all the data we have available for both training models and **testing** the models that we train.  We therefore need to split the training data into separate training and test datsets.  You will need this **test data** to evaluate your model once you are finished training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f91e8f002a63bc72069c5ac957ca5b4",
     "grade": false,
     "grade_id": "train-test-code",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "[train, test] = train_test_split(original_training_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7b9da3ae60578672c9a42db28823c0a",
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 1\n",
    "\n",
    "In the cell below, print the text of the first ham and the first spam email in the training set. Then, discuss one thing you notice that is different between the two that might relate to the identification of spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff10a1144d9c5bff85dc2274907ce32c",
     "grade": false,
     "grade_id": "q1-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: http://boingboing.net/#85534171\n",
      " date: not supplied\n",
      " \n",
      " arts and letters daily, a wonderful and dense blog, has folded up its tent due \n",
      " to the bankruptcy of its parent company. a&l daily will be auctioned off by the \n",
      " receivers. link[1] discuss[2] (_thanks, misha!_)\n",
      " \n",
      " [1] http://www.aldaily.com/\n",
      " [2] http://www.quicktopic.com/boing/h/zlfterjnd6jf\n",
      " \n",
      " \n",
      "\n",
      "<html>\n",
      " <head>\n",
      " </head>\n",
      " <body>\n",
      " <font size=3d\"4\"><b> a man endowed with a 7-8\" hammer is simply<br>\n",
      "  better equipped than a man with a 5-6\"hammer. <br>\n",
      " <br>would you rather have<br>more than enough to get the job done or fall =\n",
      " short. it's totally up<br>to you. our methods are guaranteed to increase y=\n",
      " our size by 1-3\"<br> <a href=3d\"http://209.163.187.47/cgi-bin/index.php?10=\n",
      " 004\">come in here and see how</a>\n",
      " </body>\n",
      " </html>\n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the text of the first ham and the first spam emails. Then, fill in your response in the q01 variable:\n",
    "first_ham = original_training_data[original_training_data['spam'] == 0]['email'].tolist()[0]\n",
    "first_spam = original_training_data[original_training_data['spam'] == 1]['email'].tolist()[0]\n",
    "\n",
    "print(first_ham)\n",
    "print(first_spam)\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "115ba1cae1be0d4c7a5e9b91526fcb19",
     "grade": true,
     "grade_id": "q1-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell with just a comment but don't delete me if you want to get credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc6c8ddb4b03754a64382c725dafc5b0",
     "grade": true,
     "grade_id": "q1-written",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "solution",
     "written",
     "q01"
    ]
   },
   "source": [
    "One of the main differences is formatting type. The first ham is in a plain text format while the identified spam is in HTML format. Another difference is the the links contained in the email. The ham has friendly urls while the spam has a url pointing to an IP address.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78b2a5de9976f0492e4325d88e131a47",
     "grade": false,
     "grade_id": "feat-eng",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Basic Feature Engineering\n",
    "\n",
    "We would like to take the text of an email and predict whether the text is ham or spam. This is a *classification* problem, so we can use logistic regression to make a classifier. Recall that to train an logistic regression model we need a numeric feature matrix $\\Phi$ (pronounced phi as in wifi) and corresponding binary labels $Y$.  Unfortunately, our data are text, not numbers. To address this, we can create numeric features derived from the email text and use those features for logistic regression.\n",
    "\n",
    "Each row of $\\Phi$ is derived from one email example. Each column of $\\Phi$  is one feature. We'll guide you through creating a simple feature, and you'll create more interesting ones when you are trying to increase your accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a6a1160d34bedff1e1a0859a58bf70e",
     "grade": false,
     "grade_id": "q2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 2\n",
    "\n",
    "Create a function called `words_in_texts` that takes in a list of `words` and a pandas Series of email `texts`. It should output a 2-dimensional NumPy array containing one row for each email text. The row should contain either a 0 or a 1 for each word in the list: 0 if the word doesn't appear in the text and 1 if the word does. For example:\n",
    "\n",
    "```python\n",
    ">>> words_in_texts(['hello', 'bye', 'world'], \n",
    "                   pd.Series(['hello', 'hello world hello']))\n",
    "\n",
    "array([[1, 0, 0],\n",
    "       [1, 0, 1]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "296207c542908dd11c481c4f56585743",
     "grade": false,
     "grade_id": "q2-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def words_in_texts(words, texts):\n",
    "    '''\n",
    "    Args:\n",
    "        words (list-like): words to find\n",
    "        texts (Series): strings to search in\n",
    "    \n",
    "    Returns:\n",
    "        NumPy array of 0s and 1s with shape (n, p) where n is the\n",
    "        number of texts and p is the number of words.\n",
    "    '''\n",
    "    indicator_array = []\n",
    "    for text in texts:\n",
    "        indicator_array.append([1 if word in text else 0 for word in words])\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    return np.array(indicator_array)\n",
    "\n",
    "words_in_texts(['hello', 'bye', 'world'], pd.Series(['hello', 'hello world hello']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b8eb808ad9b99843c8cd3ad5d20b736",
     "grade": true,
     "grade_id": "q2-tests",
     "locked": true,
     "points": 2,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# If this doesn't error, your function outputs the correct output for this example\n",
    "assert np.allclose(words_in_texts(['hello', 'bye', 'world'], \n",
    "                                  pd.Series(['hello', 'hello world hello'])),\n",
    "                   np.array([[1, 0, 0], \n",
    "                             [1, 0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bacca07aa9b367ec84957d0beb41886",
     "grade": false,
     "grade_id": "eda",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Basic EDA\n",
    "\n",
    "Now we need to identify some features that allow us to tell spam and ham emails apart. One idea is to compare the distribution of a single feature in spam emails to the distribution of the same feature in ham emails. If the feature is itself a binary indicator, such as whether a certain word occurs in the text, this amounts to comparing the proportion of spam emails with the word to the proportion of ham emails with the word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b13eddc109fc8ca94589381ffa188ae3",
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 3a\n",
    "\n",
    "Create a bar chart comparing the proportion of spam and ham emails containing certain words. It should look like the following plot (which was created using `sns.barplot`), but you should choose your own words as candidate features.\n",
    "\n",
    "![training conditional proportions](training_conditional_proportions.png \"Class Conditional Proportions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c64ceb5b31e1282f8c118e345108f1f2",
     "grade": true,
     "grade_id": "q3a-answer",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa94b625c18>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa94b6256d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "spam_emails = original_training_data[original_training_data['spam'] == 1]['email']\n",
    "ham_emails = original_training_data[original_training_data['spam'] == 0]['email']\n",
    "\n",
    "selected_words = ['offer','href','<html>', 'url', 'money', 'free', 'earn']\n",
    "df_spam_emails = pd.DataFrame(words_in_texts(selected_words, spam_emails), columns=selected_words).mean().transpose()\n",
    "df_ham_emails = pd.DataFrame(words_in_texts(selected_words, ham_emails), columns=selected_words).mean().transpose()\n",
    "\n",
    "df_merged = pd.DataFrame(list(df_ham_emails) + list(df_spam_emails), columns=['ratios'])\n",
    "df_merged['words'] = selected_words + selected_words\n",
    "df_merged['class'] = ['ham'] * len(list(df_ham_emails)) + ['spam'] * len(list(df_spam_emails))\n",
    "\n",
    "sns.barplot(x=\"words\", y=\"ratios\", hue=\"class\", data=df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cacc657b9f84fe89c425797255849f69",
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 3b\n",
    "\n",
    "When the feature is binary, it makes sense (as in the previous question) to compare the proportion of 1s in the two classes of email. Otherwise, if the feature can take on many values, it makes sense to compare the distribution under spam to the distribution under ham. Create a *class conditional density plot* like the one below (which was created using `sns.distplot`), comparing the distribution of a feature among all spam emails to the distribution of the same feature among all ham emails. **You may use the Fraction of Uppercase Letters or create your own feature.**\n",
    "\n",
    "![training conditional densities](training_conditional_densities2.png \"Class Conditional Densities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60246643e0caaa562f6a6ac6814ae0c0",
     "grade": true,
     "grade_id": "q3b-answer",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa94aba4048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "test_pd = pd.read_csv('data/train.csv')\n",
    "spam_upper = []\n",
    "ham_upper = []\n",
    "for email in test_pd[test_pd['spam'] == 1]['email']:\n",
    "    spam_upper.append(sum(1 for c in email if c.isupper())/len(email))\n",
    "for email in test_pd[test_pd['spam'] == 0]['email']:\n",
    "    ham_upper.append(sum(1 for c in email if c.isupper())/len(email))\n",
    "\n",
    "df_spam_u = pd.DataFrame(spam_upper, columns=['ratios'])\n",
    "df_spam_u['class'] = 'spam'\n",
    "df_ham_u = pd.DataFrame(ham_upper, columns=['ratios'])\n",
    "df_ham_u['class'] = 'ham'\n",
    "df_fracupper = pd.concat([df_spam_u,df_ham_u])\n",
    "\n",
    "sns.distplot(df_ham_u['ratios'], label='ham')\n",
    "sns.distplot(df_spam_u['ratios'], label='spam')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d13d7ef0cdd07a7ad8d0a49cdddba9b",
     "grade": false,
     "grade_id": "classification",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Basic Classification\n",
    "\n",
    "Notice that the output of `words_in_texts(words, train['email'])` is a numeric matrix containing features for each email. This means we can use it directly to train a classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a704638a2aa116d175cec5d54011390e",
     "grade": false,
     "grade_id": "q4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 4\n",
    "\n",
    "We've given you 5 words that might be useful as features to distinguish spam/ham emails. Use these words as well as the `train` DataFrame to create two NumPy arrays: `Phi_train` and `Y_train`.\n",
    "\n",
    "`Phi_train` should be a matrix of 0s and 1s created by using your `words_in_texts` function on all the emails in the training set.\n",
    "\n",
    "`Y_train` should be a vector of the correct labels for each email in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b93048015e15c7af26cd4a47e611109",
     "grade": false,
     "grade_id": "q4-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0]]), 0    0\n",
       " 1    0\n",
       " 2    1\n",
       " 3    0\n",
       " 4    0\n",
       " Name: spam, dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_words = ['drug', 'bank', 'prescription', 'memo', 'private']\n",
    "\n",
    "Phi_train = words_in_texts(some_words, original_training_data['email'])\n",
    "Y_train = original_training_data['spam']\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "Phi_train[:5], Y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e379ae3ce1967f869f7d7e731ae0f75a",
     "grade": true,
     "grade_id": "q4-tests",
     "locked": true,
     "points": 2,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.all(np.unique(Phi_train) == np.array([0, 1]))\n",
    "assert np.all(np.unique(Y_train) == np.array([0, 1]))\n",
    "assert Phi_train.shape[0] == Y_train.shape[0]\n",
    "assert Phi_train.shape[1] == len(some_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d67277515ec86e13df560be7fb273f4",
     "grade": false,
     "grade_id": "q5",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 5\n",
    "\n",
    "Now we have matrices we can give to scikit-learn! Using the [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier, train a logistic regression model using `Phi_train` and `Y_train`. Then, output the accuracy of the model (on the training data) in the cell below. You should get an accuracy of around 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a45837a9efac1b9ab3824f860e9ddc1a",
     "grade": false,
     "grade_id": "q5-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75574988021082889"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "lr = sklearn.linear_model.LogisticRegression(fit_intercept=True)\n",
    "lr.fit(Phi_train,Y_train)\n",
    "\n",
    "training_accuracy = (lr.predict(Phi_train) == Y_train).mean()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1133eaddd64442a3139615c57864c60e",
     "grade": true,
     "grade_id": "q5-tests",
     "locked": true,
     "points": 2,
     "schema_version": 2,
     "solution": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "assert training_accuracy > 0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ccbf7ec8e32a2963fac3ca5624407d0",
     "grade": false,
     "grade_id": "q6",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 6\n",
    "\n",
    "That doesn't seem too shabby! But the classifier you made above isn't as good as this might lead us to believe. First, we are evaluating on the training set, which may lead to a misleading accuracy measure, especially if we used the training set to identify discriminative features. In future parts of this analysis, it will be safer to hold out some of our data for model validation and comparison.\n",
    "\n",
    "Presumably, our classifier will be used for **filtering**, i.e. preventing messages labelled `spam` from reaching someone's inbox. Since we are trying  There are two kinds of errors we can make:\n",
    "- False positive (FP): a ham email gets flagged as spam and filtered out of the inbox.\n",
    "- False negative (FN): a spam email gets mislabelled as ham and ends up in the inbox.\n",
    "\n",
    "These definitions depend both on the true labels and the predicted labels. False positives and false negatives may be of differing importance, leading us to consider more ways of evaluating a classifier, in addition to overall accuracy:\n",
    "\n",
    "**Precision** measures the proportion $\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$ of emails flagged as spam that are actually spam.\n",
    "\n",
    "**Recall** measures the proportion $\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$ of spam emails that were correctly flagged as spam. \n",
    "\n",
    "**False-alarm rate** measures the proportion $\\frac{\\text{FP}}{\\text{FP} + \\text{TN}}$ of ham emails that were incorrectly flagged as spam. \n",
    "\n",
    "The following image might help:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"500px\">\n",
    "\n",
    "Note that a true positive (TP) is a spam email that is classified as spam, and a true negative (TN) is a ham email that is classified as ham. Answer the following questions in the cells below:\n",
    "\n",
    "- (a) Suppose we have a classifier that just predicts 0 (ham) for every email. How many false positives are there? How many false negatives are there? Provide specific numbers using the training data from Question 4.\n",
    "- (b) Suppose we have a classifier that just predicts 0 (ham) for every email. What is its accuracy on the training set? What is its recall on the training set?\n",
    "- (c) What are the precision, recall, and false-alarm rate of the logistic regression classifier in Question 5? Are there more false positives or false negatives? \n",
    "- (d) Our logistic regression classifier got 75.6% prediction accuracy (number of correct predictions / total). How does this compare with predicting 0 for every email?\n",
    "- (e) Given the word features we gave you above, name one reason this classifier is performing poorly.\n",
    "- (f) Which of these two classifiers would you prefer for a spam filter and why? (N.B. there is no \"right answer\" here but be thoughtful in your reasoning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb6be2a577f65651b583021d052505f3",
     "grade": false,
     "grade_id": "q6a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Predictor False Pos:  0\n",
      "Zero Predictor False Neg:  2140\n"
     ]
    }
   ],
   "source": [
    "# provide number of FP and FN, respectively,\n",
    "# for a classifier that always predicts 0 (never predicts positive...)\n",
    "zero_predictor_fp = sum(~Y_train & 0)\n",
    "zero_predictor_fn = sum(Y_train & ~0)\n",
    "#print(len(Y_train))\n",
    "print(\"Zero Predictor False Pos: \", zero_predictor_fp)\n",
    "print(\"Zero Predictor False Neg: \", zero_predictor_fn)\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4170e6f271d97eab6ab77523d0c9689c",
     "grade": true,
     "grade_id": "q6a-tests",
     "locked": true,
     "points": 2,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell with just a comment but don't delete me if you want to get credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "551cfb296a0d066ef7f88a4450cd56f2",
     "grade": false,
     "grade_id": "q6b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Predictor Precision:  0\n",
      "Zero Predictor Recall:  0\n"
     ]
    }
   ],
   "source": [
    "# provide training accuracy & recall, respectively,\n",
    "# for a classifier that always predicts 0\n",
    "zero_predictor_acc = 0\n",
    "zero_predictor_recall = 0\n",
    "\n",
    "print(\"Zero Predictor Precision: \", zero_predictor_acc)\n",
    "print(\"Zero Predictor Recall: \", zero_predictor_recall)\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2da0a2fd20e7773dc36190301cee53f5",
     "grade": true,
     "grade_id": "q6b-tests",
     "locked": true,
     "points": 2,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell with just a comment but don't delete me if you want to get credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3775f0397ec6a892683277ff9bb041d",
     "grade": false,
     "grade_id": "q6c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives:  137\n",
      "False Negatives:  1902\n",
      "LR Precision: = 0.6347\n",
      "LR Recall: = 0.1112\n",
      "LR False Alarm: = 0.022068\n"
     ]
    }
   ],
   "source": [
    "# provide training accuracy & recall, respectively,\n",
    "# for logistic regression classifier from question 5\n",
    "Y_pred = lr.predict(Phi_train)\n",
    "true_pos = (Y_train & Y_pred)\n",
    "false_pos = (~Y_train & Y_pred)\n",
    "false_neg = (Y_train & ~Y_pred)\n",
    "true_neg = np.logical_and(Y_pred == 0, Y_train == 0)\n",
    "\n",
    "logistic_predictor_precision = true_pos.sum() / (true_pos.sum() + false_pos.sum())\n",
    "logistic_predictor_recall = true_pos.sum() / (true_pos.sum() + false_neg.sum())\n",
    "logistic_predictor_far = false_pos.sum() / (false_pos.sum() + true_neg.sum())\n",
    "\n",
    "print(\"False Positives: \", false_pos.sum())\n",
    "print(\"False Negatives: \", false_neg.sum())\n",
    "print(f'LR Precision: = {logistic_predictor_precision:.4f}')\n",
    "print(f'LR Recall: = {logistic_predictor_recall:.4f}')\n",
    "print(f'LR False Alarm: = {logistic_predictor_far:4f}')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d5da8b3485c912e0e04d5a4cb125678",
     "grade": true,
     "grade_id": "q6c-tests",
     "locked": true,
     "points": 2,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a cell with just a comment but don't delete me if you want to get credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0c889c63a8057b74c02d98a0b25e392",
     "grade": true,
     "grade_id": "q6-written",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "solution",
     "written",
     "q06"
    ]
   },
   "source": [
    "**a.)** There are 0 false positives as the predictor predicts no positives. From the actual labels of the training data there are 2,140 false negatives with the zero predictor.  \n",
    "\n",
    "**b.)** Since the zero predictor predicts no positives, both the precision and recall are 0 on the training data.\n",
    "\n",
    "**c.)** The Logistic Regression Classifier has Precision: 0.6347, Recall 0.1112 and False Alarm Rate: 0.022068 on the training data. Logistic Regression has more False Negatives than False Positives.  \n",
    "\n",
    "**d.)** The zero predictor accuracy is slightly lower at 74%. True negatives will always be predicted and True positives will never occur.\n",
    "\n",
    "**e.)** The choice of words used maybe a reason why the classifier is performing poorly. The words selected do not have a large enough spam to ham ratio for the logistic regression to easily classify them.\n",
    "\n",
    "**f.)** Personally, having a false negative is far more preferable than having a false positive. I would choose the zero predictor in this case since, I would rather receive spam than potentially lose any legitimate emails.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "420d757256a0c1fd96228b7f622682bb",
     "grade": false,
     "grade_id": "p2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Part II - Moving Forward\n",
    "\n",
    "With this in mind, it is now your task to make the spam filter more accurate. In order to get full credit on the accuracy part of this assignment, you must get at least **88%** accuracy on the evaluation set. To see your accuracy on the evaluation set, you will use your classifier to predict every email in the `evaluation` DataFrame and upload your predictions to Kaggle.\n",
    "\n",
    "To prevent you from fitting to the evaluation set, you may only upload predictions to Kaggle twice per day. This means you should start early and rely on your **test data** to estimate your Kaggle scores.  \n",
    "\n",
    "Here are some ideas for improving your model:\n",
    "\n",
    "1. Finding better features based on the email text. Some example features are:\n",
    "    1. Number of characters in the subject / body\n",
    "    1. Number of words in the subject / body\n",
    "    1. Use of punctuation (e.g., how many '!' were there?)\n",
    "    1. Number / percentage of capital letters \n",
    "    1. Whether the email is a reply to an earlier email or a forwarded email\n",
    "1. Finding better words to use as features. Which words are the best at distinguishing emails? This requires digging into the email text itself. \n",
    "1. Better data processing. For example, many emails contain HTML as well as text. You can consider extracting out the text from the HTML to help you find better words. Or, you can match HTML tags themselves, or even some combination of the two.\n",
    "1. Model selection. You can adjust parameters of your model (e.g. the regularization parameter) to achieve higher accuracy. Recall that you should use cross-validation to do feature and model selection properly! Otherwise, you will likely overfit to your training data.\n",
    "\n",
    "You may use whatever method you prefer in order to create features. However, **you are only allowed to train logistic regression models and their regularized forms**. This means no random forest, k-nearest-neighbors, neural nets, etc.\n",
    "\n",
    "We will not give you a code skeleton to do this, so feel free to create as many cells as you need in order to tackle this task. However, answering questions 7, 8, and 9 should help guide you.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** *You should use the **test data** to evaluate your model and get a better sense of how it will perform on the Kaggle evaluation.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>20313</td>\n",
       "      <td>0.023140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>18876</td>\n",
       "      <td>0.021503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>14106</td>\n",
       "      <td>0.016069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>12525</td>\n",
       "      <td>0.014268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you</td>\n",
       "      <td>11580</td>\n",
       "      <td>0.013192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>9483</td>\n",
       "      <td>0.010803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>=</td>\n",
       "      <td>9228</td>\n",
       "      <td>0.010512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>for</td>\n",
       "      <td>8022</td>\n",
       "      <td>0.009139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>your</td>\n",
       "      <td>8014</td>\n",
       "      <td>0.009129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;td</td>\n",
       "      <td>7805</td>\n",
       "      <td>0.008891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  count     ratio\n",
       "0   the  20313  0.023140\n",
       "1    to  18876  0.021503\n",
       "2   and  14106  0.016069\n",
       "3    of  12525  0.014268\n",
       "4   you  11580  0.013192\n",
       "5     a   9483  0.010803\n",
       "6     =   9228  0.010512\n",
       "7   for   8022  0.009139\n",
       "8  your   8014  0.009129\n",
       "9   <td   7805  0.008891"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get dictionary of words for frequencies\n",
    "from collections import Counter\n",
    "\n",
    "spam_word_list = []\n",
    "ham_word_list = []\n",
    "\n",
    "for email in train[train['spam'] == 1]['email']:\n",
    "    for word in email.split():\n",
    "        spam_word_list.append(word)\n",
    "\n",
    "for email in train[train['spam'] == 0]['email']:\n",
    "    for word in email.split():\n",
    "        ham_word_list.append(word)        \n",
    "        \n",
    "spam_word_dict = Counter(spam_word_list)\n",
    "ham_word_dict = Counter(ham_word_list)\n",
    "\n",
    "spam_words = pd.DataFrame.from_dict(spam_word_dict,orient='index').sort_values(by=0,ascending=False).reset_index()\n",
    "ham_words = pd.DataFrame.from_dict(ham_word_dict,orient='index').sort_values(by=0,ascending=False).reset_index()\n",
    "spam_words.columns = ['word','count']\n",
    "ham_words.columns = ['word','count']\n",
    "\n",
    "spam_words['ratio'] = spam_words['count'] / spam_words['count'].sum()\n",
    "ham_words['ratio'] = ham_words['count'] / ham_words['count'].sum()\n",
    "\n",
    "spam_words.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>64331</td>\n",
       "      <td>0.034596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&gt;</td>\n",
       "      <td>47992</td>\n",
       "      <td>0.025809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>38625</td>\n",
       "      <td>0.020772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>29739</td>\n",
       "      <td>0.015993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>29537</td>\n",
       "      <td>0.015885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of</td>\n",
       "      <td>29008</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>in</td>\n",
       "      <td>19831</td>\n",
       "      <td>0.010665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>17363</td>\n",
       "      <td>0.009338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>for</td>\n",
       "      <td>15678</td>\n",
       "      <td>0.008431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that</td>\n",
       "      <td>14852</td>\n",
       "      <td>0.007987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  count     ratio\n",
       "0   the  64331  0.034596\n",
       "1     >  47992  0.025809\n",
       "2    to  38625  0.020772\n",
       "3     a  29739  0.015993\n",
       "4   and  29537  0.015885\n",
       "5    of  29008  0.015600\n",
       "6    in  19831  0.010665\n",
       "7    is  17363  0.009338\n",
       "8   for  15678  0.008431\n",
       "9  that  14852  0.007987"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing & Model Fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "#Creating data set\n",
    "\n",
    "def punc_in_texts(punc, texts):\n",
    "    '''\n",
    "    Args:\n",
    "        punc (list-like): punctuation to find\n",
    "        texts (Series): strings to search in\n",
    "    \n",
    "    Returns:\n",
    "        NumPy array of 0s and 1s with shape (n, p) where n is the\n",
    "        number of texts and p is the number of punctuation.\n",
    "    '''\n",
    "    indicator_array = []\n",
    "    for text in texts:\n",
    "        indicator_array.append([1 if p in text else 0 for p in punc])\n",
    "    return np.array(indicator_array)\n",
    "\n",
    "#calculates the ratio of upper to lower case\n",
    "#returns list of ratios\n",
    "non_lower_case = pd.read_csv('data/train.csv')\n",
    "[nonlc_train, nonlc_test] = train_test_split(non_lower_case, test_size=0.1, random_state=42)\n",
    "\n",
    "def uplow_ratio(data, isTest, isEval):\n",
    "    ratios = []\n",
    "    if isTest and not isEval:\n",
    "        for email in nonlc_test['email']:\n",
    "            ratios.append(sum(1 for c in email if c.isupper())/len(email))\n",
    "    elif not isTest and not isEval:\n",
    "        for email in nonlc_train['email']:\n",
    "            ratios.append(sum(1 for c in email if c.isupper())/len(email))\n",
    "    #this is evaluation data\n",
    "    else:\n",
    "        for email in evaluation['email']:\n",
    "            ratios.append(sum(1 for c in email if c.isupper())/len(email))\n",
    "            \n",
    "    return ratios\n",
    "\n",
    "#returns list of subject wordcount\n",
    "def sub_count(data):\n",
    "    modified_df = data.fillna(\" \")\n",
    "    return [len(sub.split()) for sub in modified_df['subject']]\n",
    "\n",
    "#returns list of subject wordcount\n",
    "def bod_count(data):\n",
    "    modified_df = data.fillna(\" \")\n",
    "    return [len(bod.split()) for bod in modified_df['email']]\n",
    "\n",
    "#returns list of subject length\n",
    "def sub_length(data):\n",
    "    modified_df = data.fillna(\" \")\n",
    "    return [len(sub) for sub in modified_df['subject']]\n",
    "\n",
    "#returns list of body length\n",
    "def bod_length(data):\n",
    "    modified_df = data.fillna(\" \")\n",
    "    return [len(bod) for bod in modified_df['email']]\n",
    "\n",
    "#one hot encodes if email is reply or forwarded\n",
    "def reporfor(data):\n",
    "    #fill na values\n",
    "    df = data['subject'].fillna(\" \")\n",
    "    clean_list = []\n",
    "    \n",
    "    #clean each row with just values of reply, forward or none\n",
    "    for sub in df:\n",
    "        if 'RE:' in sub:\n",
    "            clean_list.append('reply')\n",
    "        elif 'FW:' in sub:\n",
    "            clean_list.append('forward')\n",
    "        else:\n",
    "            clean_list.append('none')\n",
    "    \n",
    "    cats = ['reply','forward','none']\n",
    "    cat_type = CategoricalDtype(categories=cats)\n",
    "    \n",
    "    ret_df = pd.DataFrame(clean_list, columns=['sub_type'])\n",
    "    \n",
    "    ret_df['sub_type'] = ret_df['sub_type'].astype(cat_type)\n",
    "    ret_df = pd.get_dummies(ret_df,\n",
    "                          prefix='ohe',\n",
    "                          columns=['sub_type'], \n",
    "                          drop_first=False)\n",
    "    \n",
    "    return ret_df\n",
    "    \n",
    "    \n",
    "\n",
    "#Creates dataframe of generated features\n",
    "def process_data(data, isTest=False, isEval=False):\n",
    "    #feature ratio of upper to lower case \n",
    "    uplow_ratios = uplow_ratio(data, isTest, isEval)\n",
    "    \n",
    "    #Final DataFrame\n",
    "    final_df = pd.DataFrame(uplow_ratios, columns=['uplow_ratios'])\n",
    "    \n",
    "    #Feature selected words\n",
    "    selected_words = ['href', '<body>', '<table>', 'offer', '.org', 'url', 'click',\n",
    "                      'your', '1','our','yo','in','ont','on','wi','nt','wrote','date','time','calendar']\n",
    "    word_columns = words_in_texts(selected_words, data['email'])    \n",
    "    final_df = pd.concat([final_df, pd.DataFrame(word_columns)], axis=1)\n",
    "    \n",
    "    #Feature selected punctuation\n",
    "    selected_punc = ['!','#','?','\"','@','*','$','%','\\\\','+','=']\n",
    "    punc_columns = punc_in_texts(selected_punc, data['email'])    \n",
    "    final_df = pd.concat([final_df, pd.DataFrame(punc_columns)], axis=1)\n",
    "    \n",
    "    #Feature subject word count\n",
    "    sb_ct = sub_count(data)\n",
    "    final_df = pd.concat([final_df, pd.DataFrame(sb_ct, columns=['sub_count'])], axis=1)\n",
    "    \n",
    "    #Feature body word count\n",
    "    bd_ct = bod_count(data)\n",
    "    final_df = pd.concat([final_df, pd.DataFrame(bd_ct, columns=['bod_count'])], axis=1)\n",
    "    \n",
    "    #Feature subject length\n",
    "    sb_ln = sub_length(data)\n",
    "    final_df = pd.concat([final_df, pd.DataFrame(sb_ln, columns=['sub_length'])], axis=1)\n",
    "    \n",
    "    #Feature body length\n",
    "    bd_ln = bod_length(data)\n",
    "    final_df = pd.concat([final_df, pd.DataFrame(bd_ln, columns=['bod_length'])], axis=1)\n",
    "    \n",
    "    #Feature OHE subject type\n",
    "    ohe_sub = reporfor(data)\n",
    "    final_df = pd.concat([final_df, ohe_sub], axis=1)\n",
    "    \n",
    "    #print(final_df.head(10))\n",
    "    return final_df.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "#Final Model on Training Data\n",
    "X_train = process_data(train)\n",
    "Y_train = train['spam']\n",
    "\n",
    "#Logistic Regression with LASSO regularization\n",
    "final_lr = sklearn.linear_model.LogisticRegression(fit_intercept=True, penalty='l1')\n",
    "final_lr.fit(X_train,Y_train)\n",
    "\n",
    "training_accuracy = (final_lr.predict(X_train) == Y_train).mean()\n",
    "print(\"Training Accuracy:\", training_accuracy)\n",
    "\n",
    "\n",
    "#Final Model on Testing Data\n",
    "X_test = process_data(test, isTest=True)\n",
    "Y_test = test['spam']\n",
    "\n",
    "test_accuracy = (final_lr.predict(X_test) == Y_test).mean()\n",
    "print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous accuracies\n",
    "selected_words = ['href', '<html>', '<img>', '<div>', '<table>', 'free', 'offer', 'src=', \n",
    "                      '.org', '.org', 'url', 'click', 'your', '1','our','yo','in','ont','on','wi','nt']  \n",
    "Training Accuracy: 0.906162651404  \n",
    "Testing Accuracy: 0.900598802395  \n",
    "\n",
    "['href', '<html>', '<img', '<div', '<table', 'free', 'offer', 'src=', '.org', '.org', 'url', 'click']  \n",
    "Training Accuracy: 0.891920670837  \n",
    "Testing Accuracy: 0.892215568862  \n",
    "\n",
    "Training Accuracy: 0.892053773459\n",
    "Testing Accuracy: 0.882634730539"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55580b1682a53b3d3bab8e3d6b928973",
     "grade": false,
     "grade_id": "q7",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 7 (Feature/Model Selection Process)\n",
    "\n",
    "In this following cell, describe the process of improving your model. You should use at least 2-3 sentences each to address the follow questions:\n",
    "\n",
    "1. How did you find better features for your model?\n",
    "2. What did you try that worked / didn't work?\n",
    "3. What was surprising in your search for good features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "939e6a5a273ced9801a7c510aac0cde4",
     "grade": true,
     "grade_id": "q7-written",
     "locked": false,
     "points": 6,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "written",
     "q_feature"
    ]
   },
   "source": [
    "**1.)** Our approach to finding features was first take the recommendations for features noted. The recommended features allowed us to get fairly close to the target but features we needed to analyze the contents of each email and peform feature extractions on particular words. I created dictionaries with the counts of each word in the emails for ham and spam and calculated the proportion of occurrences for each. We then chose words with the greatest spam to ham ratio, meaning words that leaned more towards spam or ham which would be easier to classify. Each word was then encoded as a feature.  \n",
    "**2.)** Adding too many words as features had the unfortunate effect of dropping the overall testing accuracy. This is most likely due to an increase in the variance of the predictor (bias/variance tradeoff), so we started to remove any features that had the least effect on the training/test accuracy. This required much trial and error.  \n",
    "**3.)** Surprisingly many of the words that had the highest spam to ham ratio were fragments of words, or words that were inconceivable of being a contributor, such the word 'wrote' apparently is a great indicator, while certain html tags such as '<div>' performed poorly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60b1230b3da1ce9160009cccef25dd8d",
     "grade": false,
     "grade_id": "q8",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 8 (EDA)\n",
    "\n",
    "In the two cells below, show a visualization that you used to select features for your model. Include both\n",
    "\n",
    "1. A plot showing something meaningful about the data that helped you during feature / model selection.\n",
    "2. 2-3 sentences describing what you plotted and what its implications are for your features.\n",
    "\n",
    "Feel to create as many plots as you want in your process of feature selection, but select one for the cells below.\n",
    "\n",
    "**You should not show us a visualization just like in question 3.** Specifically, don't show us a bar chart of proportions, or a one-dimensional class conditional density plot. Any other plot is acceptable, as long as it comes with thoughtful commentary. Here are some ideas:\n",
    "\n",
    "1. Consider the correlation between multiple features (look up correlation plots and `sns.heatmap`). \n",
    "1. Try to show redundancy in a group of features (e.g. `body` and `html` might co-occur relatively frequently, or you might be able to design a feature that captures all html tags and compare it to these). \n",
    "1. Use a word-cloud or another visualization tool to characterize the most common spam words.\n",
    "1. Visually depict whether spam emails tend to be wordier (in some sense) than ham emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94f71f52acc1ddc1af721de03dddc841",
     "grade": true,
     "grade_id": "q8-eda",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "written",
     "q_eda1"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d76d0efe11b5d997378ac902d04f7c1",
     "grade": true,
     "grade_id": "q8-commentary",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "I chose a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99200fdbaff3f814a5c7685c862989d9",
     "grade": false,
     "grade_id": "q9",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 9 (Making a Precision-Recall Curve)\n",
    "\n",
    "We can trade off between precision and recall. In most cases we won't be able to get both perfect precision (i.e. no false positives) and recall (i.e. no false negatives), so we have to compromise. For example, in the case of cancer screenings, false negatives are comparatively worse than false positives — a false negative means that a patient might not discover a disease until it's too late to treat, while a false positive means that a patient will probably have to take another screening.\n",
    "\n",
    "Recall that logistic regression calculates the probability that an example belongs to a certain class. Then, to classify an example we say that an email is spam if our classifier gives it $\\ge 0.5$ probability of being spam. However, *we can adjust that cutoff*: we can say that an email is spam only if our classifier gives it $\\ge 0.7$ probability of being spam, for example. This is how we can trade off false positives and false negatives.\n",
    "\n",
    "The precision-recall curve shows this trade off for each possible cutoff probability. In the cell below, [plot a precision-recall curve](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#plot-the-precision-recall-curve) for your final classifier (the one you use to make predictions for Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8aed6cd26a59c471f66756ebb5e8bc51",
     "grade": true,
     "grade_id": "q9-roc-curve",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "written",
     "q_roc"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Note that you'll want to use the .predict_proba(...) method for your classifier\n",
    "# instead of .predict(...) so you get probabilities, not classes\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "#predict probability for HAM\n",
    "#final_lr.predict_proba(X_test)[:,0]\n",
    "#predict probability for SPAM\n",
    "#final_lr.predict_proba(X_test)[:,1]\n",
    "precision, recall, _ = precision_recall_curve(Y_test, final_lr.predict_proba(X_test)[:,1])\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: SPAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f241f96a5f0b08ae0392d96660d109d",
     "grade": false,
     "grade_id": "q10",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 10: Submitting to Kaggle\n",
    "\n",
    "The following code will write your predictions on the evaluation dataset to a CSV, which you can submit to Kaggle. You may need to modify it to suit your needs.\n",
    "\n",
    "Save your predictions in a 1-dimensional array called `evaluation_predictions`. *Even if you are not submitting to Kaggle, please make sure you've saved your predictions to `evaluation_predictions` as this is how your grade for this part will be determined.*\n",
    "\n",
    "Remember that if you've performed transformations or featurization on the training data, you must also perform the same transformations on the evaluation data in order to make predictions. For example, if you've created features for the words \"drug\" and \"money\" on the training data, you must also extract the same features in order to use scikit-learn's `.predict(...)` method.\n",
    "\n",
    "You should submit your CSV files to https://www.kaggle.com/t/39fae66747b14fd48fe0984f2e4f16ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a2abb910f7ef354666d7e39daed7454",
     "grade": false,
     "grade_id": "q10-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# CHANGE ME (Currently making random predictions)\n",
    "#print(\"len\", len(evaluation))\n",
    "X_eval = process_data(evaluation, isEval=True)\n",
    "evaluation_predictions = final_lr.predict(X_eval)\n",
    "#print(evaluation_predictions.shape)\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5005948ef8f43a714dad71e48c71bb8a",
     "grade": true,
     "grade_id": "q10-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# must be ndarray of predictions\n",
    "assert isinstance(evaluation_predictions, np.ndarray) \n",
    "\n",
    "# must be binary labels (0 or 1) and not probabilities\n",
    "assert np.all((evaluation_predictions == 0) | (evaluation_predictions == 1))\n",
    "\n",
    "# must be the right number of predictions\n",
    "assert evaluation_predictions.shape == (1000, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9dfffe9e2d4d757b84e8d3601b5e5dd9",
     "grade": true,
     "grade_id": "q10-hidden-tests",
     "locked": true,
     "points": 4,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Please do not modify this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f08c87d79dbcfed4134d2f56fda5b1fa",
     "grade": false,
     "grade_id": "cell-d15e30e2a961277d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "The following saves a file to submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Assuming that your predictions on the evaluation set are stored in a 1-dimensional array called\n",
    "# evaluation_predictions. Feel free to modify this cell as long you create a CSV in the right format.\n",
    "\n",
    "# must be ndarray of predictions\n",
    "assert isinstance(evaluation_predictions, np.ndarray) \n",
    "\n",
    "# must be binary labels (0 or 1) and not probabilities\n",
    "assert np.all((evaluation_predictions == 0) | (evaluation_predictions == 1))\n",
    "\n",
    "# must be the right number of predictions\n",
    "assert evaluation_predictions.shape == (1000, )\n",
    "\n",
    "# Construct and save the submission:\n",
    "submission_df = pd.DataFrame({\n",
    "    \"Id\": evaluation['id'], \n",
    "    \"Class\": evaluation_predictions,\n",
    "}, columns=['Id', 'Class'])\n",
    "timestamp = datetime.isoformat(datetime.now()).split(\".\")[0]\n",
    "submission_df.to_csv(\"submission_{}.csv\".format(timestamp), index=False)\n",
    "\n",
    "print('Created a CSV file: {}.'.format(\"submission_{}.csv\".format(timestamp)))\n",
    "print('You may now upload this CSV file to Kaggle for scoring.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
